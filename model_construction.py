import joblib
import numpy as np
from sklearn import metrics
from xgboost import XGBClassifier
from matplotlib import pyplot as plt
from sklearn.model_selection import train_test_split

data_path = '/mnt/diskd/datasets/mining'
base_path = './idata'
pic_path = './pic'


def get_xgb_oof(x_train, y_train, x_test, random_state):
    """Train a classifier that only applies Boosting(XGBoost).

    :param x_train: feature matrix of training set
    :param y_train: true labels of training set
    :param x_test: feature matrix of test set
    :param random_state: random seed to randomly set the form of input data
    :return: predicted labels of test set
    """

    model = XGBClassifier(n_estimators=50, max_depth=5, subsample=0.8, random_state=random_state,
                          use_label_encoder=False, eval_metric='logloss')
    model.fit(x_train, y_train)
    oof_test = model.predict(x_test)
    return oof_test.reshape(-1, 1)


def get_xgb_oof_advanced(x_train, y_train, x_test, random_state):
    """Train a classifier that applies Boosting and optimized strategy.(XGBoost & Bagging)

    :param x_train: feature matrix of training set
    :param y_train: true labels of training set
    :param x_test: feature matrix of test set
    :param random_state: random seed to randomly set the form of input data
    :return: predicted labels of test set
    """

    result = []
    for i in np.random.randint(0xFFFFF, size=10):
        # Use Bagging to randomly extract training data from the datasets
        train_data, test_data, train_label, test_label = train_test_split(x_train,
                                                                          y_train,
                                                                          test_size=0.2,
                                                                          random_state=i)
        model = XGBClassifier(n_estimators=50, max_depth=5, subsample=0.8, random_state=random_state,
                              use_label_encoder=False, eval_metric='logloss')
        # Use Boosting to train sub-models
        model.fit(train_data, train_label)
        y_pred = model.predict(x_test)
        result.append(y_pred)
    # Acquire final predicted result by averaging multiple results from sub-models
    y_pred = np.array(result).mean(axis=0)
    return y_pred.reshape(-1, 1)


def plot_roc_curve(y_test, y_pred_array, seeds, row, col, width, height, advanced=True):
    """Plot ROC curves for all the predicted results generated by models under different random seeds.

    :param y_test: true labels of test set
    :param y_pred_array: predicted labels of test set
    :param seeds: sequence of random seeds set in the training phase
    :param row: number of rows to display multiple figures
    :param col: number of columns to display multiple figures
    :param width: width of the whole picture to display multiple figures
    :param height: height of the whole picture to display multiple figures
    :param advanced: flag to identify if the detection model applies the optimized strategy
    """

    # Calculate the values of TPR/FPR/AUC and save them in the form of list
    fpr_list, tpr_list, auc_list = [], [], []
    for i in range(y_pred_array.shape[0]):
        fpr, tpr, _ = metrics.roc_curve(y_test, y_pred_array[i, :])
        auc = metrics.auc(fpr, tpr)
        fpr_list.append(fpr)
        tpr_list.append(tpr)
        auc_list.append(auc)

    auc_array = np.array(auc_list)

    fig, axes = plt.subplots(row, col, figsize=(width, height), tight_layout=True)
    ax0 = fig.add_subplot(111)
    ax0.spines['top'].set_color('none')
    ax0.spines['bottom'].set_color('none')
    ax0.spines['left'].set_color('none')
    ax0.spines['right'].set_color('none')
    ax0.tick_params(labelcolor='w', top='off', bottom='off', left='off', right='off')

    axes = [i for ax in axes for i in ax]

    for ax, fpr, tpr, auc, seed in zip(axes, fpr_list, tpr_list, auc_list, seeds):
        ax.plot(fpr, tpr, color="steelblue", label=f"xgb_rand_{seed}\nauc={auc:.5f}", rasterized=True, linewidth=1.5)
        ax.set_xlim([0.0, 1.0])
        ax.set_ylim([0.9, 1.05])
        ax.legend(loc="lower right", markerscale=0.1, edgecolor='1.0')

    ax0.set_xlabel("False Positive Rate", fontweight='bold', labelpad=20)
    ax0.set_ylabel("True Positive Rate", fontweight='bold', labelpad=28)
    ax0.set_title("ROC curve", fontweight='bold')
    ax0.set_facecolor('none')
    ax0.set_xticks([])
    ax0.set_yticks([])

    if advanced:
        with open(f'{base_path}/xgb_advanced_aucs.pkl', 'wb') as fp:
            joblib.dump(auc_array, fp)
        fig.savefig(f"{pic_path}/xgb_advanced.pdf", bbox_inches='tight')
    else:
        with open(f'{base_path}/xgb_aucs.pkl', 'wb') as fp:
            joblib.dump(auc_array, fp)
        fig.savefig(f"{pic_path}/xgb.pdf", bbox_inches='tight')


def plot_varied_curve():
    """Plot curves of AUC variation between XGBoost and fused model."""

    with open(f'{base_path}/xgb_aucs.pkl', 'rb') as fp:
        y_xgb = joblib.load(fp)
    with open(f'{base_path}/xgb_advanced_aucs.pkl', 'rb') as fp:
        y_xgb_adv = joblib.load(fp)

    x = np.arange(1, 10)
    x_ticks = ['1st', '2nd', '3rd']
    x_ticks += [str(i) + 'th' for i in np.arange(4, 10)]

    plt.plot(x, y_xgb, marker='s', color='r', linestyle='-', linewidth=1.5,
             label=f'Boosting-XGBoost: range={np.ptp(y_xgb):.2%}; std={np.std(y_xgb):.5f}')

    plt.plot(x, y_xgb_adv, marker='o', color='b', linestyle='-', linewidth=1.5,
             label=f'Boosting&Bagging: range={np.ptp(y_xgb_adv):.2%}; std={np.std(y_xgb_adv):.5f}')

    plt.xlim([0.5, 9.5])
    plt.ylim([0.97, 1.0])
    plt.xticks(x, x_ticks, fontweight='bold')
    plt.yticks(fontweight='bold')
    plt.legend(loc="lower left")
    plt.tick_params(axis="both")
    plt.ylabel("AUC", fontweight='bold')

    plt.title('AUC variation', fontweight='bold')
    plt.savefig(f"{pic_path}/AUC_variation.pdf", bbox_inches='tight')
